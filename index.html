<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Siddhartha Bhaumik's Portfolio</title>
    <link rel="stylesheet" href="styles.css">
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
        }
        header {
            background-color: #333;
            color: #fff;
            padding: 10px 0;
            text-align: center;
        }
        header h1 {
            margin: 0;
            font-size: 2.5em;
        }
        nav ul {
            list-style-type: none;
            padding: 0;
        }
        nav ul li {
            display: inline;
            margin-right: 20px;
        }
        nav ul li a {
            color: #fff;
            text-decoration: none;
            font-size: 1.2em;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        main {
            padding: 20px;
        }
        section {
            margin-bottom: 30px;
        }
        h2 {
            border-bottom: 2px solid #333;
            padding-bottom: 5px;
            margin-bottom: 15px;
        }
        h3 {
            margin-top: 20px;
        }
        p, ul {
            margin-bottom: 15px;
        }
        ul {
            list-style-type: disc;
            padding-left: 20px;
        }
        
    </style>
</head>
<body>
    <header>
        <h1>Siddhartha Bhaumik</h1>
        <nav>
            <ul>
                <li><a href="#about">About Me</a></li>
                <li><a href="#projects">Projects</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section id="about">
            <h2>About Me</h2>
            <p>
                <ul>I am Siddhartha Bhaumik, A highly experienced and driven Data Analytics professional with over 17 years of excellence in software development, data warehousing, and data quality initiatives. Extensive expertise in leveraging cutting-edge Big Data technologies such as HDFS, Hive, PySpark, and NiFi, as well as cloud-based services like AWS S3, EC2, DynamoDB, and Snowflake.</ul>
            </p>
            <h3>Key Strengths</h3>
            <ul>
                <li>Extensive working experience in Abinitio ETL tools, Python, Teradata, Snowflake, SQL, and Shell scripting.</li>
                <li>Proven analytical mindset with a propensity towards critical thinking and problem-solving.</li>
                <li>Quick learner with the ability to adapt quickly to new environments.</li>
                <li>Highly productive, fierce fast attitude, multi-tasking, and prioritization expert with decision-making capabilities while gathering information and demonstrating leadership to peer and mid-level staff.</li>
                <li>Extensive experience in the financial domain (Credit Card, Banking) with the ability to build and lead teams in all SD methodologies - Agile, Iterative.</li>
                <li>Solid global delivery experience with managing cross-functional teams with an incredible success rate.</li>
                <li>Outstanding organization building abilities to form and lead teams in onshore-offshore models, build solutions, and deliver on key growth targets.</li>
                <li>Strong interpersonal and communication skills with abilities to achieve results by collaborating with multiple cross-functional teams in a multi-vendor environment.</li>
            </ul>
            <h3>Specialties</h3>
            <p>
                Exceptional track record of success around leading and delivering complex business-critical applications in Data Warehousing, Data Analytics, Data Quality, and Data Governance space. Extensive experience in Credit Card Risk, Marketing, and Banking domains.
            </p>
            <h2>Professional Degree</h2>
            <ul><li>MS in Data Science - Bellevue University,NE (USA) (May 2024)</li>
            <li>B.E Electrical Engineering - BIT,CG(India) (Jun 2003)</li></ul>
                    <h2>Certifications</h2>
        <ul>
            <li>Generative AI Engineering with Databricks</li>
            <li>GPT-4 Foundations: Building AI-Powered Apps</li>
            <li>IBM Data Science Professional Certificate
                <ul>
                    <li>Python for Data Science and AI</li>
                    <li>Machine Learning with Python</li>
                    <li>Data Analysis with Python</li>
                    <li>Data Visualization with Python</li>
                    <li>Applied Data Science Capstone</li>
                </ul>
            </li>
            <li>SnowPro Core Certification (Snowflake)</li>
            <li>Data Engineering with Google Cloud Professional Certificate
                <ul>
                    <li>Google Cloud Platform Big Data and Machine Learning Fundamentals</li>
                    <li>Modernizing Data Lakes and Data Warehouses with GCP</li>
                    <li>Building Batch Data Pipelines on GCP</li>
                    <li>Building Resilient Streaming Analytics on GCP</li>
                    <li>Smart Analytics, Machine Learning, and AI on GCP</li>
                </ul>
            </li>
            <li>Hands-on Snowflake – WebUI Essentials</li>
            <li>AWS Certified Machine Learning – Specialty Course</li>
            <li>Introduction to Data Science in Python by University of Michigan on Coursera</li>
            <li>Big Data on AWS</li>
            <li>Python 3 Scripting for System Administrators</li>
            <li>AWS Certified Big Data – Specialty Course</li>
            <li>AWS Certified Developer - Associate</li>
            <li>The Python Mega Course</li>
            <li>Python for Data Science</li>
            <li>Apache Spark and Scala Developer</li>
        </ul>
            <h3>Technical Experience</h3>
            <p>
                Programming Languages: Python, SQL, Scala, R, Shell scripting,AWS services (S3, EC2, VPC, Lambda, DynamoDB) <br>
                Tools: Abinitio (ETL), Abinitio ExpressIT (DQ), Tableau,IBM Tivoli, BMC Control-M, Service Now <br>
                Version Control: Abinitio EME, Git/GitHub <br>
                Operating Systems: UNIX AIX/Linux <br>
                Databases: Teradata, Oracle 11g, Snowflake, Google BigQuery <br>
                Big Data/Hadoop: HDFS, Apache Hive, Apache Spark
            </p>
        </section>

        <section id="projects">
            <h2>Projects</h2>
            <div class="project">
                <h3>Forecasting EV Landscape</h3>
                <p>This project looks at the Electric Vehicle population and sales across the United States, exploring historical data, challenges, competition, market situation, and predicting future trends and growth in the US market.</p>
                <p><strong>Dataset used:</strong> US_Electric_fuel_vehicles.csv</p>
                <p><strong>Python Libraries used:</strong> Pandas, NumPy, Matplotlib, Seaborn, BeautifulSoup, scikit-learn</p>
                <p><strong>ML techniques used:</strong> Regression analysis, Predictive analysis, Cluster analysis, Correlation analysis, Categorical analysis</p>
                <a href="https://github.com/sidbhaumik/sidb_datascience_projects.github.io/blob/main/Forecasting%20EV%20Landscape.pdf">View Project on GitHub</a>
            </div>
            <div class="project">
                <h3>Indian Restaurants Program</h3>
                <p>The goal is to find the best US city out of 4 given choices with the highest Indian restaurant density.
                <li>Chicago, IL</li>
                <li>Miami,FL</li>
                <li>New York, NY</li>
                <li>Los Angeles,CA</li></p>
                <p><strong>Method used:</strong> I have used the Four Square API through the venues channel. I used the near query to get venues in the cities. Also, I have used the CategoryID 4bf58dd8d48988d10f941735 to show only Indian restaurantsFoursquare limits us to maximum of 100 venues per query. I repeated this request for the 4 U.S cities and got their top 100 venues. I only saved the name and coordinate data from the result and plotted them on the map using Folium for visual inspection.</p>
                <a href="https://github.com/sidbhaumik/sidb_datascience_projects.github.io/blob/main/Indian_restaurants_program.ipynb">View Project on GitHub</a>
            </div>
            <div class="project">
                <h3>Exploring Neighborhoods in the city of Toronto</h3>
                <p>Exploring Toronto neighborhood. Generate maps to visualize the neighborhoods and how they cluster together.</p>
                <p><strong>Dataset used:</strong> <a href="https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M">Wikipedia Postal Codes</a>, <a href="http://cocl.us/Geospatial_data">Geospatial Data</a></p>
                <p><strong>Python Libraries used:</strong> Pandas, NumPy, Matplotlib, Seaborn, BeautifulSoup, sklearn, Folium</p>
                <p><strong>Method used:</strong>
                    <ul>
                        <li>Data is scraped from Wikipedia to get a list of postal codes and neighborhoods in Toronto.</li>
                        <li>Using the Foursquare API, nearby venues for each neighborhood are obtained and categorized.</li>
                        <li>The neighborhoods are clustered into 5 groups based on the types of venues present, using K-means clustering.</li>
                        <li>The most common venue categories for each neighborhood are identified and tabulated.</li>
                        <li>Geographic coordinates are obtained for each neighborhood using geocoding.</li>
                        <li>An interactive Folium map is generated, with markers indicating the neighborhoods color-coded by their cluster assignment.</li>
                    </ul>
                </p>
                 <a href="https://github.com/sidbhaumik/sidb_datascience_projects.github.io/blob/main/Exploring_Neighborhoods_in_the_city_of_Toronto.pdf">View Project on GitHub</a>
            </div>
            <div class="project">
                <h3>Boston Housing Market Analysis</h3>
                <p>This document analyzes the Boston housing market dataset.</p>
                <p><strong>Dataset used:</strong> Boston_housing.csv</p>
                <p><strong>Python Libraries used:</strong> Pandas, NumPy, Matplotlib</p>
                <p><strong>Method used:</strong>
                    <ul>
                        <li>The Boston housing dataset is read from a CSV file, containing information on variables like crime rate, age of homes, rooms per dwelling, etc.</li>
                        <li>Histograms are plotted for all variables in the new DataFrame using a loop.</li>
                        <li>Scatter plots are created to visualize the relationship between crime rate (and its log value) vs housing prices.</li>
                        <li>Some useful statistics are calculated:
                            <ul>
                                <li>Mean age of houses.</li>
                                <li>Maximum age of houses.</li>
                                <li>Proportion of houses over 80 years old.</li>
                                <li>Mean crime rate of houses with prices above average and below average.</li>
                            </ul>
                        </li>
                        <li>Conclusions are drawn regarding the relationship between crime rate and housing prices.</li>
                    </ul>
                </p>
                <a href="https://github.com/sidbhaumik/sidb_datascience_projects.github.io/blob/main/Boston%20Housing%20Market.pdf">View Project on GitHub</a>
            </div>
            <div class="project">
                <h3>Movie Database</h3>
                <p>Building a movie database.</p>
                <p><strong>Dataset used:</strong> http://www.omdbapi.com/?</p>
                <p><strong>Python Libraries used:</strong> urllib, json</p>
                <p><strong>Method used:</strong>
                    <ul>
                        <li>1. It imports necessary libraries like urllib, json.</li>
                        <li>2. It loads a secret API key from a JSON file for the OMDb (Open Movie Database) API</li>
                        <li>3. It defines utility functions to:Print movie data from a JSON response.Download and save a movie poster locally.Search for a movie by name, print its details, and save the poster</li>
                        <li>4. It tests the search functionality with movies like "Titanic" and an invalid name.</li
                    </ul>
                </p>
                <a href="https://github.com/sidbhaumik/sidb_datascience_projects.github.io/blob/main/Movie_database_program.pdf">View Project on GitHub</a>
            </div>
      <div class="project">
    <h3>Predicting Fuel Efficiency</h3>
    <p>This document focuses on building a linear regression model to predict fuel efficiency (miles per gallon) of automobiles using the auto-mpg dataset.</p>
    <p><strong>Dataset used:</strong> auto-mpg.csv</p>
    <p><strong>Python Libraries used:</strong> pandas, seaborn, matplotlib, scipy, sklearn</p>
    <p><strong>Method used:</strong>
        <ul>
            <li><strong>Data Loading and Preprocessing:</strong>
                <ul>
                    <li>The dataset is loaded into a Pandas dataframe.</li>
                    <li>Exploratory data analysis is performed, including checking correlations and creating visualizations.</li>
                    <li>Missing values in the 'horsepower' column are handled.</li>
                    <li>The data is split into training and testing sets.</li>
                </ul>
            </li>
            <li><strong>Linear Regression Model:</strong>
                <ul>
                    <li>A linear regression model is trained on the training data.</li>
                    <li>Performance metrics like RMSE (root mean squared error), MAE (mean absolute error), and R-squared are calculated for both training and testing data.</li>
                    <li>The coefficients of the linear regression model are printed.</li>
                </ul>
            </li>
            <li><strong>XGBoost Regression Model:</strong>
                <ul>
                    <li>An XGBoost regression model is built using GridSearchCV for hyperparameter tuning.</li>
                    <li>The XGBoost model's performance is evaluated using RMSE, MAE, and R-squared on both training and testing data.</li>
                </ul>
            </li>
            <li><strong>Results and Comparison:</strong>
                <ul>
                    <li>The XGBoost model performs better than the linear regression model, with lower RMSE and MAE values on both training and testing data.</li>
                    <li>The R-squared value for the XGBoost model is slightly higher than the linear regression model, indicating a better fit.</li>
                </ul>
            </li>
        </ul>
    </p>
    <a href="https://github.com/sidbhaumik/sidb_datascience_projects.github.io/blob/main/Predicting%20Fuel%20Efficiency.pdf">View Project on GitHub</a>
</div>
            <div class="project">
    <h3>Recommendation System</h3>
    <p>This movie recommendation program allows users to get personalized movie recommendations based on their input.</p>
    <p><strong>Dataset used:</strong> movies.csv, ratings.csv</p>
    <p><strong>Python Libraries used:</strong> pandas, fuzzywuzzy, sklearn</p>
    <p><strong>Method used:</strong> The code implements a movie recommender system using the MovieLens dataset, which includes movie ratings and titles. The main steps are:
        <ul>
            <li>Load and merge the ratings and movies data into a single DataFrame.</li>
            <li>Create a pivot table with movie titles as rows, user IDs as columns, and ratings as values.</li>
            <li>Fill in missing rating values with 0.</li>
            <li>Create a TF-IDF vector representation of the movie titles.</li>
            <li>Calculate a cosine similarity matrix between the movie title vectors.</li>
            <li>Define a function to get the closest matching movie title based on fuzzy string matching.</li>
            <li>Define a function to generate top N recommendations for a given movie title by finding the most similar movies based on the cosine similarity matrix.</li>
        </ul>
    </p>
    <p>The recommender system allows the user to input a movie title, and it will:
        <ul>
            <li>Find the closest matching movie title.</li>
            <li>Use the cosine similarity to find the top 10 most similar movies.</li>
            <li>Print the recommended movie titles.</li>
        </ul>
    </p>
    <p>The key aspects of the recommender system are preprocessing the data, creating a matrix of movie similarities, and using fuzzy matching to handle user input.</p>
    <a href="https://github.com/sidbhaumik/sidb_datascience_projects.github.io/blob/main/Recommender%20System.pdf">View Project on GitHub</a>
</div>
<div class="project">
    <h3>Sentiment Analysis</h3>
    <p>This code demonstrates how to perform sentiment analysis on movie reviews using two different libraries (TextBlob and VADER), as well as how to create bag-of-words and TF-IDF representations of the text data, which can be useful for further machine learning tasks.</p>
    <p><strong>Dataset used:</strong> labeledTrainData.tsv</p>
    <p><strong>Python Libraries used:</strong> pandas, numpy, nltk, textblob</p>
    <p><strong>Method used:</strong>
        <ul>
            <li><strong>Data Preprocessing:</strong> The code reads in a TSV dataset containing movie reviews and their corresponding sentiment labels. It cleans the text by converting to lowercase, removing punctuation, and tokenizing the words. It also removes stop words and applies stemming to the text.</li>
            <li><strong>Sentiment Analysis using TextBlob:</strong> The code uses the TextBlob library to generate a sentiment polarity score for each review, ranging from -1 to 1. It then categorizes the reviews as "POSITIVE" or "NEGATIVE" based on the polarity score. The accuracy of the TextBlob-based sentiment analysis is calculated and reported.</li>
            <li><strong>Sentiment Analysis using VADER:</strong> The code uses the VADER (Valence Aware Dictionary and sEntiment Reasoner) library to generate sentiment scores for each review. It categorizes the reviews as "POSITIVE" or "NEGATIVE" based on the compound sentiment score. The accuracy of the VADER-based sentiment analysis is calculated and reported.</li>
            <li><strong>Bag-of-Words Representation:</strong> The code creates a bag-of-words matrix from the preprocessed text using the CountVectorizer from scikit-learn. It also creates a TF-IDF (Term Frequency-Inverse Document Frequency) matrix from the preprocessed text. The dimensions of the bag-of-words and TF-IDF matrices are displayed.</li>
        </ul>
    </p>
    <a href="https://github.com/sidbhaumik/sidb_datascience_projects.github.io/blob/main/Sentiment_Analysis.pdf">View Project on GitHub</a>
</div>
<div class="project">
    <h3>Top 100 ebooks titles</h3>
    <p>This code demonstrates how to scrape data (top 100 ebook titles) from a website (Project Gutenberg) using Python and common libraries like BeautifulSoup and regex.</p>
    <p><strong>Dataset used:</strong> <a href="https://www.gutenberg.org/browse/scores/top">https://www.gutenberg.org/browse/scores/top</a></p>
    <p><strong>Python Libraries used:</strong> urllib.request, urllib.parse, urllib.error, requests, BeautifulSoup</p>
    <p><strong>Method used:</strong>
        <ul>
            <li>The code extracts the top 100 ebooks from the Project Gutenberg website.</li>
            <li>It follows these main steps:
                <ul>
                    <li>Ignores SSL certificate errors to access the website.</li>
                    <li>Reads the HTML content of the top 100 ebooks page.</li>
                    <li>Finds all the href links on the page.</li>
                    <li>Uses regular expressions to extract the numeric file numbers for the top 100 ebooks.</li>
                    <li>Extracts the titles of the top 100 ebooks from the HTML content.</li>
                </ul>
            </li>
            <li>Specifically, the code:
                <ul>
                    <li>Loops through the href links to find the file numbers for the top 100 ebooks.</li>
                    <li>Searches the HTML content to find the section with the "Top 100 EBooks yesterday" heading.</li>
                    <li>Extracts the titles of the top 100 ebooks by looping through the lines of text after the heading and using regular expressions to remove the non-title text.</li>
                    <li>Prints the list of the top 100 ebook titles.</li>
                </ul>
            </li>
            <li>The key aspects of the code are:
                <ul>
                    <li>Handling the SSL certificate issue to access the website.</li>
                    <li>Using BeautifulSoup to parse the HTML content.</li>
                    <li>Leveraging regular expressions to extract the desired information (file numbers and titles).</li>
                    <li>Iterating through the HTML content to find the relevant sections and extract the data.</li>
                </ul>
            </li>
        </ul>
    </p>
    <a href="https://github.com/sidbhaumik/sidb_datascience_projects.github.io/blob/main/Top100_ebooks_program.pdf">View Project on GitHub</a>
</div>
<div class="project">
    <h3>Weather App</h3>
    <p>The code implements a simple and user-friendly weather application that retrieves and displays current weather and 36-hour forecast data for any US city or zip code using the OpenWeatherMap API.</p>
    <p><strong>Dataset used:</strong>
        <ul>
            <li><a href="https://api.openweathermap.org/data/2.5/weather">https://api.openweathermap.org/data/2.5/weather</a></li>
            <li><a href="https://api.openweathermap.org/data/2.5/forecast">https://api.openweathermap.org/data/2.5/forecast</a></li>
        </ul>
    </p>
    <p><strong>Python Libraries used:</strong> requests, json, time</p>
    <p><strong>Method used:</strong>
        <ul>
            <li>The main functionality is contained in the <code>main()</code> function, which:
                <ul>
                    <li>Prompts the user to enter a zip code or city/country name.</li>
                    <li>Calls two helper functions to retrieve the current weather data and the extended 36-hour forecast data.</li>
                    <li>Displays the current weather information and the 36-hour forecast.</li>
                    <li>Prompts the user to enter another location or exit the program.</li>
                </ul>
            </li>
            <li>The <code>weather_current()</code> function:
                <ul>
                    <li>Constructs the API request URL based on the user input (zip code or city/country).</li>
                    <li>Makes a GET request to the OpenWeatherMap API to retrieve the current weather data.</li>
                    <li>Handles any errors that may occur during the request.</li>
                    <li>Formats and prints the current weather information.</li>
                </ul>
            </li>
            <li>The <code>weather_extended()</code> function:
                <ul>
                    <li>Constructs the API request URL for the 36-hour forecast based on the user input.</li>
                    <li>Makes a GET request to the OpenWeatherMap API to retrieve the extended forecast data.</li>
                    <li>Handles any errors that may occur during the request.</li>
                    <li>Formats and prints the 36-hour forecast information.</li>
                </ul>
            </li>
            <li>The <code>convert_temp()</code> function is a helper function that converts temperatures from Kelvin to Fahrenheit and Celsius.</li>
            <li>The <code>try_web()</code> function is a helper function that handles various types of errors that may occur during the API requests, such as HTTP errors, connection errors, timeout errors, and other exceptions.</li>
            <li>The <code>current_formatted()</code> and <code>ext_formatted()</code> functions format the current weather and extended forecast data, respectively, for easy display to the user.</li>
            <li>The <code>more_weather()</code> function allows the user to enter another location or exit the program.</li>
        </ul>
    </p>
    <a href="https://github.com/sidbhaumik/sidb_datascience_projects.github.io/blob/main/Weather%20App.pdf">View Project on GitHub</a>
</div>


        </section>

        <section id="contact">
            <h2>Contact</h2>
            <p>You can reach me at:</p>
            <ul>
                <li>LinkedIn: <a href="https://www.linkedin.com/in/siddhartha-bhaumik-98a28019/">Siddhartha Bhaumik</a></li>
                <li>Email: <a href="mailto:sid.bhaumik@gmail.com">siddhartha bhaumik</a></li>
                
                <li>GitHub: <a href="https://github.com/sidbhaumik">sidbhaumik</a></li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2024 Siddhartha Bhaumik. All rights reserved.</p>
    </footer>
</body>
</html>
